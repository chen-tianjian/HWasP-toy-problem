import gym

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # only show warning and errors in TF
import tensorflow as tf
os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # force cpu
import numpy as np

from garage.envs import normalize
from garage.experiment import run_experiment
from garage.tf.algos.ppo import PPO
from garage.tf.baselines import GaussianMLPBaseline
from garage.np.baselines import LinearFeatureBaseline

from garage.tf.envs import TfEnv
from garage.tf.experiment import LocalTFRunner
from garage.tf.models.mlp_model import MLPModel
from garage.experiment import SnapshotConfig, deterministic

import dowel

import cma

from mass_spring_envs.envs.mass_spring_env_opt_k import MassSpringEnv_OptK_HwAsAction
from policies.opt_k.models import MechPolicyModel_OptK_HwAsAction
from policies.opt_k.policies import CompMechPolicy_OptK_HwAsAction

from shared_params import params_opt_k as params

from launchers.utils.zip_project import zip_project
# from launchers.utils.normalized_env import normalize

from datetime import datetime
import sys
import argparse



def obj_fcn(k_init, snapshot_config):
    """Run task."""
    with LocalTFRunner(snapshot_config=snapshot_config, sess=tf.compat.v1.Session()) as runner:

        params.k_init = params.inv_sigmoid(k_init, params.k_lb, params.k_ub)

        # env = TfEnv(normalize(MassSpringEnv_OptK_HwAsAction(params), normalize_action=False, normalize_obs=False, normalize_reward=True, reward_alpha=0.1))

        env = TfEnv(MassSpringEnv_OptK_HwAsAction(params))

        comp_policy_model = MLPModel(output_dim=1, 
            hidden_sizes=params.comp_policy_network_size, 
            hidden_nonlinearity=tf.nn.tanh,
            output_nonlinearity=tf.nn.tanh,
            )

        mech_policy_model = MechPolicyModel_OptK_HwAsAction(params)

        policy = CompMechPolicy_OptK_HwAsAction(name='comp_mech_policy', 
                env_spec=env.spec, 
                comp_policy_model=comp_policy_model, 
                mech_policy_model=mech_policy_model)

        # baseline = GaussianMLPBaseline(
        #     env_spec=env.spec,
        #     regressor_args=dict(
        #         hidden_sizes=params.baseline_network_size,
        #         hidden_nonlinearity=tf.nn.tanh,
        #         use_trust_region=True,
        #     ),
        # )
        
        baseline = LinearFeatureBaseline(env_spec=env.spec)

        algo = PPO(
            env_spec=env.spec,
            policy=policy,
            baseline=baseline,
            **params.ppo_algo_kwargs
        )

        runner.setup(algo, env)
        average_return = runner.train(**params.ppo_inner_train_kwargs)
        runner.sess.close()

    tf.reset_default_graph()

    return average_return

    
if __name__=='__main__':

    now = datetime.now()

    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', default=int(now.timestamp()), type=int, help='seed')
    parser.add_argument('--exp_id', default=now.strftime("%Y_%m_%d_%H_%M_%S"), help='experiment id (suffix to data directory name)')

    args = parser.parse_args()

    # run_experiment(run_task, exp_prefix='ppo_opt_k_hw_as_action_{}_'.format(args.exp_id) + str(params.n_springs)+'_params', snapshot_mode='last', seed=args.seed, force_cpu=True)

    snapshot_config = SnapshotConfig(
            snapshot_dir=os.path.join(os.environ['PROJECTDIR'], 'data/local/', 'cmaes_ppo_opt_k_hw_as_action_{}_'.format(args.exp_id) + str(params.n_springs)+'_params'),
            snapshot_mode='last', 
            snapshot_gap=None)

    deterministic.set_seed(args.seed)

    # CMA-ES global optimization
    options = params.cmaes_options
    x0 = params.cmaes_x0
    sigma0 = params.cmaes_sigma0

    es = cma.CMAEvolutionStrategy(x0, sigma0, options)
    es.optimize(obj_fcn, args=[snapshot_config])
    es.result_pretty()

    zip_project(log_dir=snapshot_config.snapshot_dir)